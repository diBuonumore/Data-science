{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c111cf",
   "metadata": {},
   "source": [
    "## Australian rainfall prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c62b743",
   "metadata": {},
   "source": [
    "Goal: **To predict the next-day rain based on other atmospheric features**\n",
    "    \n",
    "Dataset: This dataset comprises a decade of daily weather observations from multiple locations across Australia. \n",
    "\n",
    "Source: https://www.kaggle.com/datasets/arunavakrchakraborty/australia-weather-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3116c547",
   "metadata": {},
   "source": [
    "### Data Description:\n",
    "    \n",
    "**Location** - Name of the city from Australia.\n",
    "\n",
    "**MinTemp/MaxTemp** - The minimum/maximum temperature during a particular day. (degree Celsius)\n",
    "\n",
    "**Rainfall** - Rainfall during a particular day. (millimeters)\n",
    "\n",
    "**Evaporation** - Evaporation during a particular day. (millimeters)\n",
    "\n",
    "**Sunshine** - Bright sunshine during a particular day. (hours)\n",
    "\n",
    "**WindGusDir** - The direction of the strongest gust during a particular day. (16 compass points)\n",
    "\n",
    "**WindGuSpeed** - Speed of strongest gust during a particular day. (kilometers per hour)\n",
    "\n",
    "**WindDir9am / WindDir3pm** - The direction of the wind for 10 min prior to 9 am. / 3pm. (compass points)\n",
    "\n",
    "**WindSpeed9am / WindSpeed3pm** - Speed of the wind for 10 min prior to 9 am. / 3pm. (kilometers per hour)\n",
    "\n",
    "**Humidity9am / Humidity3pm** - The humidity of the wind at 9 am. / 3pm. (percent)\n",
    "\n",
    "**Pressure9am / Pressure3pm** - Atmospheric pressure at 9 am. / 3pm. (hectopascals)\n",
    "\n",
    "**Cloud9am / Cloud3pm** - Cloud-obscured portions of the sky at 9 am. / 3pm.(eighths)\n",
    "\n",
    "**Temp9am / Temp3pm** - The temperature at 9 am. / 3pm.(degree Celsius)\n",
    "\n",
    "**RainToday** - If today is rainy then ‘Yes’. If today is not rainy then ‘No’.\n",
    "\n",
    "**RainTomorrow** - If tomorrow is rainy then 1 (Yes). If tomorrow is not rainy then 0 (No).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">The variables below were not presented in original dataset. However, I thought it would be wise to add regions and coordinates in order to plot the cities on the map (which I prepared in Tableau). Always better to see where the places  we are taking about are.\n",
    "\n",
    "**State/Province** - State/Province of the locations in Australia\n",
    "\n",
    "**Longitute/Latitude** - Coordinates of mentioned cities\n",
    "</div>\n",
    "\n",
    "It's a pity that there is no information about the date, however when we plot some variables I'm sure we will receive some seasonal trends in the data for example in min max temperature. \n",
    "\n",
    "Why do we measure \"clouds\", \"wind direction\",\"humidity\",\"temperature\" at 9am and 3pm? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4307aea",
   "metadata": {},
   "source": [
    "It is also important to become acquainted with Australian climate especially in respect of rainfall before we will immerse in the analysis of our dataset. Here are some key aspects:\n",
    "    \n",
    "1. Australia is located in the Southern Hemisphere and is surrounded by oceans (Southern Ocean,Pacific Ocean, Indian Ocean) and seas (Timor Sea,Arafura Sea,Coral Sea, Tasman Sea).\n",
    "2. Dry and arid regions particularly in the central and western regions. These areas experience hot and dry conditions for much of the year, with limited rainfall. They are characterized by vast deserts, such as the Simpson Desert and the Great Victoria Desert.\n",
    "3. Northern parts of Australia have tropical / subtropical climate - wet/dry seasons; monsoon seasons (wet) - from November to April,heavy rainfall,tropical cyclones; dry seasons - from May to October, lower humidity, clear skies\n",
    "4. Southern / Southeastern regions have temparate climate - mild winters, moderate summers, rainfall throughout the year\n",
    "5. Southern / Southwestern parts have mediterranean climate - mild, wet winters and hot,dry summers\n",
    "6. Apline climate in southeastern regions (particularly the Australian Alps) - cold winters,snowfall,cool summers, higher rainfall due to orographic effects(rainfall caused by the lifting of moist air over mountains)\n",
    "7. Coastale areas have mild temperatures and high humidity\n",
    "8. Australia is prone to natural disasters, including tropical cyclones in the north and bushfires in various parts of the country.\n",
    "\n",
    "To conclude: Australia's climate is highly diverse and can be broadly classified into several distinct regions based on precipitation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from scipy.stats import chi2_contingency\n",
    "# from scipy.stats import kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e420fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv(\"Weather Data.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfcd99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_data = raw_data.copy()\n",
    "weather_data = weather_data.drop(['row ID'],axis=1)\n",
    "weather_data.info()\n",
    "# 25 columns, 99516 rows\n",
    "#there are some nulls \n",
    "#types: object, float64,int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c44ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317eec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) #to see all the columns; None - meaning there will be no limit to the number of columns displayed\n",
    "weather_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "NA_data = weather_data.isnull().sum()\n",
    "NA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_percentage = round(NA_data / weather_data.shape[0] *100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915d5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of missing values in each column: \\n{}\".format(NA_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insights: \n",
    "#1. Are the missing values clustered in specific regions? \n",
    "#2. Better to start from numeric or categorical features? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192701d",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "Common strategies: \n",
    "\n",
    "    1. remove rows/columns with NaNs - however this can lead to a loss of valuable information\n",
    "    2. use statistical methods - for time-series data -> interpolation,extrapolation\n",
    "    3. instead of filling in missing values, we can create an additional binary column to indicate whether a value is missing or not\n",
    "    4. Imputation - replace missing values with estimated/predicted values\n",
    "\n",
    "There are several methods to fill gaps in a categorical variable:\n",
    "\n",
    "1. Mode Imputation: Replace the missing values with the most frequent category (mode) in the variable. \n",
    "\n",
    "2. Random Imputation: Replace the missing values with random samples from the distribution of the existing categories in the variable. \n",
    "\n",
    "3. Backward Fill (bfill) and Forward Fill (ffill): Fill the missing values with the previous (bfill) or next (ffill) non-missing value in the variable - when the data follows a temporal or sequential pattern.\n",
    "\n",
    "4. K-Nearest Neighbors (KNN) Imputation: based on the similarity of other data points. \n",
    "\n",
    "5. Hot Deck Imputation: Replace missing values with values randomly selected from similar records based on certain criteria, such as matching values of other features.\n",
    "\n",
    "6. Imputation Using Machine Learning Models like decision trees or random forests to predict the missing categories based on other features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb6647",
   "metadata": {},
   "source": [
    "# Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf48a6",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>Location - nominal</span>\n",
    "#### <span style='color: #A78295;'>Name of the city from Australia </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['Location'].value_counts()\n",
    "#Why are there the most observations from Canberra,Sydney,Perth,Hobart..? Why this kind of hirarchy? How did they get this proportions?\n",
    "#Well.. Firstly, these cities are the biggest, the most populous. Secondly, I noticed that the first listed cities are located in different regions on the map. \n",
    "#We distinguish 9 states/provinces, and the first 7 cities belongs to the 7 different regions. Circumstance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(weather_data['Location'].value_counts()) #pandas.core.series.Series\n",
    "#weather_data['Location'].value_counts().index\n",
    "#weather_data['Location'].value_counts().index[:10]\n",
    "#weather_data['Location'].value_counts()[:10]\n",
    "\n",
    "location_counts = weather_data['Location'].value_counts()\n",
    "first_names = location_counts.index[:10]\n",
    "first_counts = location_counts[:10]\n",
    "\n",
    "sns.barplot(x=first_names, y=first_counts,palette='viridis')\n",
    "\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 10 Locations by count')\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "sns.despine()  # Remove the top and right spines\n",
    "\n",
    "# Add annotations for each bar\n",
    "for index, value in enumerate(first_counts):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "    \n",
    "# e.g.if first_counts is [10, 25, 15, 30, 20], the enumerate() function will yield the following tuples: (0, 10), (1, 25), (2, 15), (3, 30), and (4, 20).\n",
    "# index: The x-coordinate at which the text will be placed\n",
    "# value: The y-coordinate at which the text will be placed\n",
    "# str(value): The text that will be displayed at the specified coordinates\n",
    "# ha - horizontal alignment\n",
    "# va - vertical alignment\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6be35a",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>State/Province - nominal</span>\n",
    "#### <span style='color: #A78295;'>State or province of the cities in Australia </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd80347",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['State/Province'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts = weather_data['State/Province'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))  #specifies the width and height of the figure in inches\n",
    "sns.set_palette('tab20b')  \n",
    "\n",
    "plt.pie(state_counts, labels=state_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# autopct='%1.1f%%' - automatic percentage -> to display the percentage of each wedge(category)\n",
    "# f - float, 1.1 - one digit after decimal point, %% - escape sequence to display % symbol\n",
    "# By default, the first wedge starts from the positive x-axis (0 degrees) and proceeds counterclockwise - startangle\n",
    "\n",
    "plt.title('State/Province Distribution')\n",
    "\n",
    "plt.axis('equal')  # Equal aspect ratio ensures the pie is circular\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247451a",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>WindGustDir - nominal</span>\n",
    "#### <span style='color: #A78295;'>The direction of the strongest gust during a particular day. (16 compass points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb444ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "weather_data[weather_data['WindGustDir'].isnull()].head(100)\n",
    "#In rows with missing WindGustDirection there are also NaN values in other variables\n",
    "#NaN values in WindGustDir represent 6.6% of total number of rows, this is not so much, so that I decided to remove rows \n",
    "#with missing values in WindGustDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_cleaned_gust_dir = weather_data.copy()\n",
    "weather_data_cleaned_gust_dir = weather_data_cleaned_gust_dir.dropna(subset=['WindGustDir'])\n",
    "NA_data_cleaned_gust_dir = weather_data_cleaned_gust_dir.isnull().sum()\n",
    "NA_data_cleaned_gust_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we removed the following number of missing observations:\n",
    "NA_data - NA_data_cleaned_gust_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_cleaned_gust_dir['WindGustDir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_order = ['N','NNE','NE','ENE','E','ESE','SE','SSE','S','SSW','SW','WSW','W','WNW','NW','NNW']\n",
    "reversed_custom_order = custom_order[::-1]\n",
    "wind_gust_dir = weather_data_cleaned_gust_dir['WindGustDir'].value_counts().reindex(reversed_custom_order)\n",
    "wind_gust_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9165874",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))  #specifies the width and height of the figure in inches\n",
    "sns.set_palette('tab20b')  \n",
    "\n",
    "plt.pie(wind_gust_dir, labels=wind_gust_dir.index, autopct='%1.1f%%', startangle=100)\n",
    "\n",
    "plt.title('The direction of the strongest gust during a particular day - frequency',pad=20) # pad=20 to make space between the title and the pie chart\n",
    "\n",
    "plt.axis('equal')  # Equal aspect ratio ensures the pie is circular\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65471a0c",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>WindDir9am / WindDir3pm - nominal</span>\n",
    "#### <span style='color: #A78295;'>The direction of the wind for 10 min prior to 9 am. / 3pm. (compass points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing NaN rows in WindGustDir, there is NaNs rows in: \n",
    "# WindDir9am         5265\n",
    "# WindDir3pm          668\n",
    "weather_data_cleaned_gust_dir.dropna(subset=['WindDir9am']).isnull().sum()\n",
    "#I want to check if I remove missing values from WindDir9am, the missing values from WindDir3pm will disappear too. But only around 200 records contained NANs in both variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_percentage = round(NA_data_cleaned_gust_dir / weather_data_cleaned_gust_dir.shape[0] *100,1)\n",
    "print(\"Percentage of missing values in each column: \\n{}\".format(NA_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3622fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before filling null values\n",
    "weather_data_dir = weather_data_cleaned_gust_dir.copy()\n",
    "wind_dir_9am = weather_data_dir['WindDir9am'].value_counts().reindex(custom_order)\n",
    "wind_dir_3pm = weather_data_dir['WindDir3pm'].value_counts().reindex(custom_order)\n",
    "wind_dir = pd.DataFrame({'Direction': custom_order,'10 min prior to 9 am': wind_dir_9am.values, '10 min prior to 3 pm':wind_dir_3pm.values})\n",
    "wind_dir.set_index('Direction', inplace=True)\n",
    "ax = wind_dir.plot(kind='bar', figsize=(10, 6), width=0.5,color=['#3F4E4F', '#DCD7C9'], edgecolor='black')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_xlabel('Wind Direction')\n",
    "ax.set_title('Wind Direction Comparison (9am vs. 3pm)')\n",
    "plt.legend(title='Time', fontsize=10)\n",
    "plt.xticks(rotation=45)\n",
    "#plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e92e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After filling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c66bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A convenient solution in this case may be to replace NaNs with the most frequent category (mode) since we have only few missing values\n",
    "#type(weather_data_cleaned['WindDir9am'].mode()) pandas.core.series.Series\n",
    "weather_data_dir_with_mode = weather_data_dir.copy()\n",
    "mode_winddir9am = weather_data_dir_with_mode['WindDir9am'].mode().iloc[0]\n",
    "mode_winddir3pm = weather_data_dir_with_mode['WindDir3pm'].mode().iloc[0]\n",
    "print(f\"The mode in WindDri9am is: {mode_winddir9am}, while in WindDir3pm: {mode_winddir3pm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_dir_with_mode['WindDir9am'] = weather_data_dir_with_mode['WindDir9am'].fillna(mode_winddir9am)\n",
    "weather_data_dir_with_mode['WindDir3pm'] = weather_data_dir_with_mode['WindDir3pm'].fillna(mode_winddir3pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_dir_with_mode.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48426557",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dir_9am = weather_data_dir_with_mode['WindDir9am'].value_counts().reindex(custom_order)\n",
    "wind_dir_3pm = weather_data_dir_with_mode['WindDir3pm'].value_counts().reindex(custom_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59292a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doubled_custom_order = [dir for dir in custom_order for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcadc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wind_dir = pd.DataFrame({'Direction': custom_order,'10 min prior to 9 am': wind_dir_9am.values, '10 min prior to 3 pm':wind_dir_3pm.values})\n",
    "wind_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ee541",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dir.set_index('Direction', inplace=True)\n",
    "wind_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa32bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grouped barplot\n",
    "ax = wind_dir.plot(kind='bar', figsize=(10, 6), width=0.5,color=['#3F4E4F', '#DCD7C9'], edgecolor='black')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_xlabel('Wind Direction')\n",
    "ax.set_title('Wind Direction Comparison (9am vs. 3pm)')\n",
    "plt.legend(title='Time', fontsize=10)\n",
    "plt.xticks(rotation=45)\n",
    "#plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc47934",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>Cloud9am / Cloud3pm - ordinal</span>\n",
    "#### <span style='color: #A78295;'>Cloud-obscured portions of the sky at 9 am. / 3pm.(scale in oktas - eighths)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e914a5",
   "metadata": {},
   "source": [
    "In meteorology, cloud cover is often measured in \"oktas.\" An okta is a unit of measurement used to estimate the fraction of the sky covered by clouds at any given time. It is divided into 8 equal parts, with each okta representing 1/8th of the sky covered by clouds.However, we can distinguish also 0 oktas and 9 oktas. \n",
    "\n",
    "Here's how cloud cover is typically described in terms of oktas:\n",
    "\n",
    "0 oktas: Completely clear sky, no clouds.\n",
    "\n",
    "1 okta: Very few clouds, almost clear sky.\n",
    "\n",
    "2 oktas: Partly cloudy, 25% of the sky covered by clouds.\n",
    "\n",
    "3 oktas: Mostly cloudy.\n",
    "\n",
    "4 oktas: More clouds, about half of the sky covered.\n",
    "\n",
    "5 oktas: Overcast sky.\n",
    "\n",
    "6 oktas: Cloudy, with about 75% of the sky covered by clouds.\n",
    "\n",
    "7 oktas: Mostly covered.\n",
    "\n",
    "8 oktas: Completely overcast, the entire sky covered by clouds.\n",
    "\n",
    "9 oktas: Sky obscured by thick clouds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_clouds = weather_data_dir_with_mode.copy()\n",
    "#weather_data_clouds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_clouds[['Cloud9am','Cloud3pm']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(weather_data_clouds['Cloud3pm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(weather_data_clouds['Cloud9am'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_clouds[['Cloud9am','Cloud3pm']].isnull().sum() / weather_data_clouds.shape[0] * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245551ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cloud_9am = weather_data_clouds['Cloud9am'].value_counts() \n",
    "weather_cloud_3pm = weather_data_clouds['Cloud3pm'].value_counts()\n",
    "weather_cloud_9am.index = weather_cloud_9am.index.astype(int)\n",
    "weather_cloud_3pm.index = weather_cloud_3pm.index.astype(int)\n",
    "clouds = pd.DataFrame({'Clouds at 9am': weather_cloud_9am, 'Clouds at 3pm':weather_cloud_3pm})\n",
    "ax = clouds.plot(kind='bar', figsize=(10, 6), width=0.5,color=['#DBDFEA', '#ACB1D6'], edgecolor='black')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_xlabel('Cloud cover')\n",
    "ax.set_title('Cloudiness measurement at 9am and 3pm')\n",
    "plt.legend(title='Time', fontsize=10)\n",
    "plt.xticks(rotation=0)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41752d9",
   "metadata": {},
   "source": [
    "## Random Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud9am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ef9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Cloud9am', data=weather_data_clouds)\n",
    "plt.xlabel('Cloud9am')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cloud9am Value Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1789e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_categorical_gaps(data,col):\n",
    "#     if data[col].dtype == 'object':  # Check if the column is categorical\n",
    "#         missing_indices = data[col].isnull()\n",
    "#         non_missing_data = data.loc[~missing_indices, col]\n",
    "#         probabilities = non_missing_data.value_counts(normalize=True)\n",
    "#         num_missing = missing_indices.sum()\n",
    "        \n",
    "#         # Generate random values based on the probabilities\n",
    "        \n",
    "#         random_values = np.random.choice(probabilities.index, size=num_missing, p=probabilities.values)\n",
    "#         data.loc[missing_indices, col] = random_values\n",
    "\n",
    "#     return data\n",
    "\n",
    "# fill_categorical_gaps(weather_data_clouds,'Cloud9am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_indices = weather_data_clouds['Cloud9am'].isnull()\n",
    "num_missing = missing_indices.sum()\n",
    "num_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_missing_data = weather_data_clouds.loc[~missing_indices,'Cloud9am' ]\n",
    "non_missing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = non_missing_data.value_counts(normalize=True) # the relative frequencies (probabilities) instead of the actual counts.\n",
    "random_values = np.random.choice(probabilities.index, size=num_missing, p=probabilities.values)\n",
    "weather_data_clouds.loc[missing_indices,'Cloud9am'] = random_values\n",
    "weather_data_clouds['Cloud9am'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9807da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#After\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Cloud9am', data=weather_data_clouds)\n",
    "plt.xlabel('Cloud9am')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cloud9am Value Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00604e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cloud 3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Cloud3pm', data=weather_data_clouds)\n",
    "plt.xlabel('Cloud9am')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cloud9am Value Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_indices = weather_data_clouds['Cloud3pm'].isnull()\n",
    "num_missing = missing_indices.sum()\n",
    "non_missing_data = weather_data_clouds.loc[~missing_indices,'Cloud3pm']\n",
    "\n",
    "probabilities = non_missing_data.value_counts(normalize=True) # the relative frequencies (probabilities) instead of the actual counts.\n",
    "random_values = np.random.choice(probabilities.index, size=num_missing, p=probabilities.values)\n",
    "\n",
    "weather_data_clouds.loc[missing_indices,'Cloud3pm'] = random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Cloud3pm', data=weather_data_clouds)\n",
    "plt.xlabel('Cloud3pm')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cloud3pm Value Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ed13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0badeed8",
   "metadata": {},
   "source": [
    "#### I have tried also with other methods,namely with the mode and RandomForestClassifier but I was not satisfied with the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65251e9",
   "metadata": {},
   "source": [
    "#### Ultimately, I chose filling NaNs with random values based on the distribution of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc4779",
   "metadata": {},
   "source": [
    "## Filling gaps with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004aed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_data_clouds = weather_data_dir_with_mode.copy()\n",
    "# clouds_9am_mode = weather_data_clouds['Cloud9am'].mode().iloc[0]\n",
    "# clouds_3pm_mode = weather_data_clouds['Cloud3pm'].mode().iloc[0]\n",
    "\n",
    "# weather_data_clouds['Cloud9am']= weather_data_clouds['Cloud9am'].fillna(clouds_9am_mode)\n",
    "# weather_data_clouds['Cloud9am']= weather_data_clouds['Cloud9am'].fillna(clouds_9am_mode)\n",
    "\n",
    "# weather_cloud_9am_with_mode = weather_data_clouds['Cloud9am'].value_counts()\n",
    "# weather_cloud_3pm_with_mode = weather_data_clouds['Cloud3pm'].value_counts()\n",
    "\n",
    "# clouds_with_mode = pd.DataFrame({'Clouds at 9am': weather_cloud_9am_with_mode, 'Clouds at 3pm':weather_cloud_3pm_with_mode})\n",
    "# clouds_with_mode\n",
    "\n",
    "# ax = clouds_with_mode.plot(kind='bar', figsize=(10, 6), width=0.5,color=['#DBDFEA', '#ACB1D6'], edgecolor='black')\n",
    "# ax.set_ylabel('Values')\n",
    "# ax.set_xlabel('Cloud cover')\n",
    "# ax.set_title('Cloudiness measurement at 9am and 3pm \\n after filling NaNs with mode')\n",
    "# plt.legend(title='Time', fontsize=10)\n",
    "# plt.xticks(rotation=0)\n",
    "# sns.despine()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9d631",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "Random Forests are relatively resistant to null values and can handle missing data implicitly during the tree-building process.\n",
    "RandomForestClassifier is a supervised learning algorithm - the model is trained on labeled data -> \n",
    "the input data (features) and their corresponding output labels (target) are provided during the training process.\n",
    "\n",
    "Inputs (features) have to be converted to numeric - for categorical features I will use the target encoding (one-hot enocoding will cause too many classes)\n",
    "Will it be good idea to combine target encoding with one-hot encoding? \n",
    "\n",
    "Outputs should remain categorical \n",
    "\n",
    "Target Leakage: \"Ensure that the target encoding is calculated based only on the training set and not using any information from the validation or test sets.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecfd1e",
   "metadata": {},
   "source": [
    "Actually, from the very beginning I wanted to use RandomForestClassifier algorithm (method that consist of multiple Decision Trees), however it gave me less than 50% accuracy on test data and validation data. I tried to improve it by experimenting with: \n",
    "1) hyperparametrs like number of decision trees (n_estimators), maximum depth of trees(max_depth),minimum number of samples required to split a node (min_samples_split).\n",
    "2) cross-validation \n",
    "3) class weights - it is recommended to set the class_weight parameter to \"balanced\" in RFC if there is class imbalance in the target variable\n",
    "4) I was thinking about using one of the hyperparameter optimization techniques (e.g., grid search, random search, Bayesian optimization) to automatically search for the optimal number of trees, but for some reasons I could not run it. \n",
    "5) smoothing -> to enhence the target encoding process: to handle rare categories\n",
    "\n",
    "I think **overfitting** is a major problem here but somehow I can't figure out the reason of it. I tried to optimalize both target encoding and RFC model and there goes something wrong. For now, I have no idea what. The code that I've tried is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57094ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_encoded = weather_data_clouds.copy()\n",
    "\n",
    "# Conversion into categorical types (because in fact RainTomorrow, Cloud9am/3pm are categorical)\n",
    "data_encoded['RainTomorrow'] = data_encoded['RainTomorrow'].map({0: 'No', 1: 'Yes'})\n",
    "cloud_categories = {\n",
    "    0: 'Clear',\n",
    "    1: 'Few Clouds',\n",
    "    2: 'Partly Cloudy',\n",
    "    3: 'Mostly Cloudy',\n",
    "    4: 'Cloudy',\n",
    "    5: 'Overcast',\n",
    "    6: 'Obscured',\n",
    "    7: 'Mostly Obscured',\n",
    "    8: 'Completely Overcast',\n",
    "    9: 'Sky Obscured'\n",
    "}\n",
    "data_encoded['Cloud3pm'] = data_encoded['Cloud3pm'].map(cloud_categories).astype('category')\n",
    "\n",
    "# Remove all rows with NaNs + remember to reset the indexes, later their might be problematic !!\n",
    "data_encoded = data_encoded.dropna(how='any')\n",
    "data_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "target_column = 'Cloud9am'\n",
    "categorical_columns_to_encode = ['Location', 'State/Province', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow', 'Cloud3pm']\n",
    "\n",
    "X = data_encoded.drop(['Cloud9am'], axis=1)\n",
    "y = data_encoded['Cloud9am']\n",
    "\n",
    "# Scale features/inputs - without dummies !!\n",
    "\n",
    "X_unscaled = X.drop(columns=categorical_columns_to_encode)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_unscaled)\n",
    "X_scaled = scaler.transform(X_unscaled)\n",
    "X_scaled = pd.DataFrame(X_scaled,columns=X_unscaled.columns)\n",
    "X = pd.concat([X_scaled, X[categorical_columns_to_encode]], axis=1)\n",
    "\n",
    "# Split the data into train,validation,train -> proportion: 70:10:20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)\n",
    "\n",
    "# Target encoding - watch that you fit only once, later you just use transform function on different data\n",
    "\n",
    "for col in categorical_columns_to_encode:\n",
    "    te = TargetEncoder(smoothing=0.2)\n",
    "    te.fit(X_train[col], y_train)\n",
    "    X_train[col] = te.transform(X_train[col])\n",
    "    X_test[col] = te.transform(X_test[col])\n",
    "    X_val[col] = te.transform(X_val[col])\n",
    "    \n",
    "# RandomForestClassifier\n",
    "\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "class_weights = 'balanced'\n",
    "\n",
    "rf_classifier = RandomForestClassifier(class_weight = class_weights,max_depth = 40, min_samples_split = 5,random_state = 5)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "training_accuracy = rf_classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", training_accuracy)\n",
    "\n",
    "validation_accuracy = rf_classifier.score(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", validation_accuracy)\n",
    "\n",
    "test_accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb4954",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation on the random forest classifier\n",
    "cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cbc9c",
   "metadata": {},
   "source": [
    "## Learning curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e992ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Create learning curve with the random forest classifier\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    rf_classifier, X_train, y_train, cv=5, train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation of training and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Training Accuracy\", color=\"blue\")\n",
    "plt.fill_between(\n",
    "    train_sizes,\n",
    "    train_mean - train_std,\n",
    "    train_mean + train_std,\n",
    "    alpha=0.1,\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.plot(train_sizes, test_mean, label=\"Validation Accuracy\", color=\"red\")\n",
    "plt.fill_between(\n",
    "    train_sizes,\n",
    "    test_mean - test_std,\n",
    "    test_mean + test_std,\n",
    "    alpha=0.1,\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24c27b",
   "metadata": {},
   "source": [
    "## Attempt to find the best hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of values for max_depth and min_samples_split to try\n",
    "#max_depth_range = [10, 20, 30, None]  # None means no maximum depth, allowing trees to grow freely\n",
    "max_depth_range = [10, 20, 30, None]\n",
    "min_samples_split_range = [2, 5, 10, 20]\n",
    "\n",
    "best_validation_accuracy = 0\n",
    "best_max_depth = None\n",
    "best_min_samples_split = None\n",
    "\n",
    "for max_depth in max_depth_range:\n",
    "    for min_samples_split in min_samples_split_range:\n",
    "        rf_classifier = RandomForestClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=5)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        validation_accuracy = rf_classifier.score(X_val, y_val)\n",
    "        print(f\"Max Depth: {max_depth}, Min Samples Split: {min_samples_split}, Validation Accuracy: {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133060e",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the hyperparameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "# }\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Perform the grid search on the training set\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best hyperparameters and model from the grid search\n",
    "# best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# # Evaluate the best model on the validation set\n",
    "# validation_accuracy = best_rf_classifier.score(X_val, y_val)\n",
    "# print(\"Best Validation Accuracy:\", validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f32db0",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f86158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = X.columns\n",
    "importances = rf_classifier.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1] #sort indexes in descending order\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances[sorted_indices], y=[features[i] for i in sorted_indices])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88d434",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A confusion matrix typically has four main components:\n",
    "\n",
    "True Positive (TP): The number of instances that are correctly predicted as positive by the model.\n",
    "\n",
    "False Positive (FP): The number of instances that are incorrectly predicted as positive by the model when they are actually negative.\n",
    "\n",
    "True Negative (TN): The number of instances that are correctly predicted as negative by the model.\n",
    "\n",
    "False Negative (FN): The number of instances that are incorrectly predicted as negative by the model when they are actually positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3eb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.DataFrame({'Actual Positive': ['TP', 'FN'], 'Actual Negative': ['FP', 'TN']}, index=['Predicted Positive', 'Predicted Negative'])\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371e3cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "confusion_matrix(y_test,y_pred)\n",
    "# It shows how many values are correct classified (those value are in the diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25dc15",
   "metadata": {},
   "source": [
    "## Classification_report\n",
    "Precision: The ability of the model to correctly identify instances of a given class. It is the ratio of true positive predictions to the total predicted positive instances.\n",
    "\n",
    "Recall: The ability of the model to correctly identify all instances of a given class. It is the ratio of true positive predictions to the total actual positive instances.\n",
    "\n",
    "F1-score: The harmonic mean of precision and recall, providing a balanced measure between precision and recall.\n",
    "\n",
    "Support: The number of samples in each class.\n",
    "\n",
    "Accuracy: The overall accuracy of the model, which is the ratio of correct predictions to the total number of samples.\n",
    "\n",
    "Macro avg: The average of the metrics (precision, recall, F1-score) for all classes. It gives equal importance to each class.\n",
    "\n",
    "Weighted avg: The weighted average of the metrics, taking into account the support (number of samples) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91fb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59499e7e",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>RainToday - nominal</span>\n",
    "#### <span style='color: #A78295;'>If today is rainy then ‘Yes’. If today is not rainy then ‘No’.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "weather_data_after_clouds = weather_data_clouds.copy()\n",
    "weather_data_after_clouds['RainToday'].head(5)\n",
    "weather_data_after_clouds['RainToday'].value_counts() #does not contain NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_after_clouds['RainToday'].isnull().sum()/weather_data_after_clouds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to remove all missing values, because they constitute only ~ 0.009%\n",
    "weather_data_after_clouds = weather_data_after_clouds[weather_data_after_clouds['RainToday'].notnull()]\n",
    "# 2nd way \n",
    "# weather_data_after_clouds= weather_data_after_clouds.dropna(subset=['RainToday'])\n",
    "weather_data_after_clouds['RainToday'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_after_clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "sns.countplot(x='RainToday', data=weather_data_after_clouds, hue='RainToday',  order=['Yes', 'No'],dodge=False)\n",
    "\n",
    "plt.xlabel('Rain Today')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Rain Today')\n",
    "\n",
    "plt.legend(title='Rain', loc='upper right', labels=['Yes', 'No'])\n",
    "\n",
    "# Adjust figure size for better visualization\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37701194",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_after_clouds['RainToday'] = weather_data_after_clouds['RainToday'].map({'Yes':1,'No':0}) \n",
    "weather_data_after_clouds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_rain = weather_data_after_clouds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f930c3",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> OUR MAIN PURPOSE </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ebb58",
   "metadata": {},
   "source": [
    "## <span style='color: #3F2E3E;'>RainTomorrow - nominal</span>\n",
    "#### <span style='color: #A78295;'>If tomorrow is rainy then 1. If tomorrow is not rainy then 0 </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dd52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_data_rain['RainTomorrow'].head(5)\n",
    "weather_data_rain['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_rain['RainToday'].value_counts() # Very similar to Rain Tomorrow -> Rain Today is a key variable for tomorrow rain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_rain['RainTomorrow'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "sns.countplot(x='RainTomorrow', data=weather_data_rain, hue='RainTomorrow',dodge=False,order=[1,0])\n",
    "\n",
    "plt.xlabel('Rain Today')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Rain Tomorrow')\n",
    "\n",
    "plt.legend(title='Rain', loc='upper right', labels=['Yes', 'No'])\n",
    "\n",
    "# Set custom labels for x-axis, position 0 -> yes, position 1 -> no \n",
    "plt.xticks(ticks=[0, 1], labels=['Yes', 'No'])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af912607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5937e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d25498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02198d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a433d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cc40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d01e1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1726fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47e6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc7799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac9be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee32d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc08145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdad5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b0440",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbb743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42c0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a502de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1462a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e4408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed771871",
   "metadata": {},
   "source": [
    "### The chi-square test - categorical vs. categorical variable\n",
    "The Chi-square test is used to determine if there is a significant association between two categorical variables. \n",
    "Process for conducting a Chi-square test:\n",
    "1. Formulate the null and alternative hypotheses:\n",
    "Null hypothesis: There is no association between the two categorical variables.\n",
    "Alternative hypothesis: There is a significant association - || - \n",
    "2. Set the significance level (alpha): Choose a significance level (commonly 0.05) that represents the threshold for determining statistical significance. The significance level determines how strong the evidence against the null hypothesis must be before we reject it.\n",
    "3. Create a contingency table (cross-tabulation)- a table that shows the observed frequencies for each combination of the two categorical variables.\n",
    "4. Compute the Chi-square test statistic - you calculate the difference between the observed and expected frequencies and then you sum up them.\n",
    "5. Find p-value\n",
    "6. Comparison: If the p-value is < 0.05, reject the null hypothesis - there is a significant association between the variables. If The p-value >= 0.05 reject alternative hypothesis - there is no significant association.\n",
    "7. Chi-square statistic: The bigger difference, the stronger association between the variables. When the test statistic is large, it suggests that there is a significant discrepancy between the observed and expected frequencies, indicating that the variables are dependent.\n",
    "\n",
    "#### I added also Cramer's V, which is an extension of the chi-square test and provides a standardized measure of association.\n",
    "Cramer's V ranges from 0 to 1, where 0 indicates no association between the categorical variables, and 1 indicates a perfect association. The formula for Cramer's V is:\n",
    "\n",
    "$$\n",
    "V = \\sqrt{\\frac{\\chi^2}{(n \\cdot min (k-1,r-1)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\chi^2$  is the chi-square statistic obtained from the chi-square test of independence between the two categorical variables.\n",
    "\n",
    "n is the total number of observations in the contingency table.\n",
    "\n",
    "k is the number of rows in the contingency table.\n",
    "\n",
    "r is the number of columns in the contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86fc03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Problem 1) Cloud's type is by default float, so I need to change it:\n",
    "cloud_categories = {\n",
    "    0: 'Clear',\n",
    "    1: 'Few Clouds',\n",
    "    2: 'Partly Cloudy',\n",
    "    3: 'Mostly Cloudy',\n",
    "    4: 'Cloudy',\n",
    "    5: 'Overcast',\n",
    "    6: 'Obscured',\n",
    "    7: 'Mostly Obscured',\n",
    "    8: 'Completely Overcast',\n",
    "    9: 'Sky Obscured'\n",
    "}\n",
    "\n",
    "cloud_9am = weather_data_clouds['Cloud9am'].map(cloud_categories).astype('category')\n",
    "cloud_3pm =  weather_data_clouds['Cloud3pm'].map(cloud_categories).astype('category')\n",
    "\n",
    "#other categorical variables\n",
    "location = weather_data_clouds['Location']\n",
    "state = weather_data_clouds['State/Province']\n",
    "wind_gust = weather_data_clouds['WindGustDir']\n",
    "wind_9am = weather_data_clouds['WindDir9am']\n",
    "wind_3pm = weather_data_clouds['WindDir3pm']\n",
    "rain_today = weather_data_clouds['RainToday']\n",
    "\n",
    "# 'RainTomorrow' - 0 if will be raining 1 if not \n",
    "rain_tomorrow = weather_data_clouds['RainTomorrow'].map({0:'No',1:'Yes'})\n",
    "# Create a crosstabulation between \"cloud\" and another categorical variable\n",
    "#pd.crosstab(cloud_9am,cloud_3pm)\n",
    "#pd.crosstab(cloud_9am,location)\n",
    "#pd.crosstab(cloud_9am,state)\n",
    "#pd.crosstab(cloud_9am,wind_gust)\n",
    "#pd.crosstab(cloud_9am,wind_9am)\n",
    "#pd.crosstab(cloud_9am,wind_3pm)\n",
    "#pd.crosstab(cloud_9am,rain_today)\n",
    "#pd.crosstab(cloud_9am,rain_tomorrow)\n",
    "\n",
    "# We might assume that if the sky is completely overcast (2) or mostly obscured (5) there is a high probability that will be raining, \n",
    "# however \"correlation does not imply causation\". It means that just because two variables show a statistical relationship (correlation) does not necessarily mean that one variable causes the other to change.\n",
    "\n",
    "pd.crosstab(cloud_3pm,rain_today)\n",
    "\n",
    "# # Perform the chi-square test\n",
    "location = weather_data_clouds['Location']\n",
    "state = weather_data_clouds['State/Province']\n",
    "wind_gust = weather_data_clouds['WindGustDir']\n",
    "wind_9am = weather_data_clouds['WindDir9am']\n",
    "wind_3am = weather_data_clouds['WindDir3pm']\n",
    "rain_today = weather_data_clouds['RainToday']\n",
    "\n",
    "categorical_tab = np.array([location,state,wind_gust,wind_9am,wind_3am,cloud_9am,cloud_3pm,rain_today,rain_tomorrow])\n",
    "categorical_tab_str = np.array(['location','state','wind_gust','wind_9am','wind_3am','cloud_9am','cloud_3pm','rain_today','rain_tomorrow'])\n",
    "\n",
    "def chi_square_test(categorical_var_1, categorical_var_2):\n",
    "    crosstab = pd.crosstab(categorical_var_1, categorical_var_2)\n",
    "    chi2, p, _, _ = chi2_contingency(crosstab)\n",
    "    n = crosstab.sum().sum()\n",
    "    # crosstab.sum() - the sum of elements along each column of the contingency table\n",
    "    # crosstab.sum().sum() - the sum of all elements in the 1-dimensional array obtained from the previous step\n",
    "    num_rows = crosstab.shape[0]\n",
    "    num_cols = crosstab.shape[1]\n",
    "    cramer_v = np.sqrt(chi2 / (n * min(num_rows - 1, num_cols - 1)))\n",
    "    return chi2, p, cramer_v\n",
    "\n",
    "pd.crosstab(cloud_9am, cloud_3pm)\n",
    "chi_square_test(cloud_9am, cloud_3pm)\n",
    "\n",
    "def chi_square_contigency(tab_x, tab_x_str, var):\n",
    "    df = pd.DataFrame(columns=['Variable', 'Chi-square', 'P-value',\"Cramer's V\"])\n",
    "    for x, x_str in zip(tab_x, tab_x_str):\n",
    "        chi2, p, cramer_v = chi_square_test(var, x)\n",
    "        result = pd.DataFrame({'Variable': [x_str], 'Chi-square': [round(chi2,2)], 'P-value': [p], \"Cramer's V\":[round(cramer_v,2)]})\n",
    "        df = pd.concat([df, result], ignore_index=True)\n",
    "        df = df.sort_values(\"Cramer's V\",ascending=False)\n",
    "    return df\n",
    "\n",
    "chi_square_contigency(categorical_tab,categorical_tab_str,cloud_9am)\n",
    "chi_square_contigency(categorical_tab,categorical_tab_str,cloud_3pm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909f942",
   "metadata": {},
   "source": [
    "**In summary**, the results show that all the variables (cloud_3pm, location, rain_tomorrow, rain_today, state, wind_9am, wind_gust, wind_3am) have a significant association with the variable \"cloud_9am.\". \n",
    "The same with \"cloud_3pm\", large chi-squares and small p-values (close to 0) indicate strong evidence against the null hypothesis and prove that there is a significant association between \"cloud_3pm\" and other variables. Moreover,The strength of association varies, with some variables exhibiting stronger associations (Cramer's V close to 1) and others having weaker associations (Cramer's V closer to 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873f60a",
   "metadata": {},
   "source": [
    "# To measure the dependency between a categorical variable and a numerical variable, we may use varoius methods:\n",
    "1. ANOVA (Analysis of Variance)-  for differences in means between multiple groups of a categorical variable with respect to a numerical variable.\n",
    "It assesses whether there is a statistically significant difference in the means of the numerical variable across the different categories.\n",
    "However, ANOVA assumes that the numerical variable follows a normal distribution within each group. \n",
    "\n",
    "2. Kruskal-Wallis Test: is a non-parametric test that can be used when the assumption of normality is violated (is suitable when the numerical variable is not normally distributed.\n",
    "It tests whether the median of the numerical variable differs significantly across the categories of the categorical variable.\n",
    "                                                                                                                \n",
    "3. Point-Biserial Correlation / Biserial Correlation: if the categorical variable is binary (only two classes)\n",
    "There are also other options but those are the most popular.  \n",
    "                                                                                                                \n",
    "I decided to use Kruskal-Wallis Test because it is safe option here. \n",
    "                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colorhunt.co/palettes/vintage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
